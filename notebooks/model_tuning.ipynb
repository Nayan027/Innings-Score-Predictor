{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab2244b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:09:28,985 - INFO - Splitting dataframe into train and test sets\n",
      "2025-04-11 13:09:29,045 - INFO - Applied Feature Engineering separately to train and test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as nayanparvez90\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as nayanparvez90\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:09:30,095 - INFO - Accessing as nayanparvez90\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"nayanparvez90/Innings-Score-Predictor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"nayanparvez90/Innings-Score-Predictor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:09:31,011 - INFO - Initialized MLflow to track repo \"nayanparvez90/Innings-Score-Predictor\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository nayanparvez90/Innings-Score-Predictor initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository nayanparvez90/Innings-Score-Predictor initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:09:31,023 - INFO - Repository nayanparvez90/Innings-Score-Predictor initialized!\n",
      "2025/04/11 13:09:31 INFO mlflow.tracking.fluent: Experiment with name 'Model-Hyper-Parameter Tuning' does not exist. Creating a new experiment.\n",
      "2025-04-11 13:09:33,508 - INFO - Training and tuning LinearRegression...\n",
      "c:\\Users\\dell\\anaconda3\\envs\\ipl_project\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "3 fits failed out of a total of 12.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dell\\anaconda3\\envs\\ipl_project\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dell\\anaconda3\\envs\\ipl_project\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\dell\\anaconda3\\envs\\ipl_project\\lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 647, in fit\n",
      "    self.coef_ = optimize.nnls(X, y)[0]\n",
      "  File \"c:\\Users\\dell\\anaconda3\\envs\\ipl_project\\lib\\site-packages\\scipy\\optimize\\_nnls.py\", line 93, in nnls\n",
      "    raise RuntimeError(\"Maximum number of iterations reached.\")\n",
      "RuntimeError: Maximum number of iterations reached.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "2025-04-11 13:09:37,335 - INFO - Best LinearRegression R² Score: 0.5745472096919254\n",
      "2025-04-11 13:09:37,335 - INFO - Best LinearRegression Parameters: {'fit_intercept': False, 'n_jobs': -1, 'positive': False}\n",
      "2025-04-11 13:09:38,241 - INFO - LinearRegression Evaluation - MAE: 12.753979391171558, MSE: 283.53222814064554, R² Score: 0.6824177443008725\n",
      "c:\\Users\\dell\\anaconda3\\envs\\ipl_project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|██████████| 7/7 [00:01<00:00,  6.25it/s]\n",
      "2025-04-11 13:09:55,811 - INFO - LinearRegression logged successfully in MLflow.\n",
      "2025/04/11 13:09:56 INFO mlflow.tracking._tracking_service.client: 🏃 View run LinearRegression_Run at: https://dagshub.com/nayanparvez90/Innings-Score-Predictor.mlflow/#/experiments/2/runs/c2c91dcddd8940b291737945814c2e5c.\n",
      "2025/04/11 13:09:56 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://dagshub.com/nayanparvez90/Innings-Score-Predictor.mlflow/#/experiments/2.\n",
      "2025-04-11 13:09:58,528 - INFO - Training and tuning Lasso...\n",
      "2025-04-11 13:10:04,292 - INFO - Best Lasso R² Score: 0.5863241642834451\n",
      "2025-04-11 13:10:04,292 - INFO - Best Lasso Parameters: {'alpha': 0.1, 'fit_intercept': True, 'max_iter': 1000, 'positive': False}\n",
      "2025-04-11 13:10:05,157 - INFO - Lasso Evaluation - MAE: 12.683223474910529, MSE: 282.3525384771161, R² Score: 0.6837391055684274\n",
      "Downloading artifacts: 100%|██████████| 7/7 [00:04<00:00,  1.55it/s]\n",
      "2025-04-11 13:10:22,837 - INFO - Lasso logged successfully in MLflow.\n",
      "2025/04/11 13:10:23 INFO mlflow.tracking._tracking_service.client: 🏃 View run Lasso_Run at: https://dagshub.com/nayanparvez90/Innings-Score-Predictor.mlflow/#/experiments/2/runs/111f62b588d746acad8ac9f6a4b3c663.\n",
      "2025/04/11 13:10:23 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://dagshub.com/nayanparvez90/Innings-Score-Predictor.mlflow/#/experiments/2.\n",
      "2025-04-11 13:10:25,523 - INFO - Training and tuning Ridge...\n",
      "2025-04-11 13:10:29,742 - INFO - Best Ridge R² Score: 0.5769983692761226\n",
      "2025-04-11 13:10:29,742 - INFO - Best Ridge Parameters: {'alpha': 10.0, 'fit_intercept': False, 'max_iter': 1000, 'solver': 'auto'}\n",
      "2025-04-11 13:10:30,907 - INFO - Ridge Evaluation - MAE: 12.788783772393664, MSE: 285.08484391887816, R² Score: 0.680678671376716\n",
      "Downloading artifacts: 100%|██████████| 7/7 [00:06<00:00,  1.16it/s]\n",
      "2025-04-11 13:10:49,915 - INFO - Ridge logged successfully in MLflow.\n",
      "2025/04/11 13:10:50 INFO mlflow.tracking._tracking_service.client: 🏃 View run Ridge_Run at: https://dagshub.com/nayanparvez90/Innings-Score-Predictor.mlflow/#/experiments/2/runs/b4b18e0d9fbb4bd3a51872347d321334.\n",
      "2025/04/11 13:10:50 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://dagshub.com/nayanparvez90/Innings-Score-Predictor.mlflow/#/experiments/2.\n",
      "Downloading artifacts: 100%|██████████| 7/7 [00:07<00:00,  1.01s/it]\n",
      "2025-04-11 13:11:16,820 - INFO - Final best model (Lasso) logged successfully in MLflow.\n",
      "2025/04/11 13:11:17 INFO mlflow.tracking._tracking_service.client: 🏃 View run Best_Model_Run at: https://dagshub.com/nayanparvez90/Innings-Score-Predictor.mlflow/#/experiments/2/runs/b99476919d9b4a4bb3e6652546bfd165.\n",
      "2025/04/11 13:11:17 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://dagshub.com/nayanparvez90/Innings-Score-Predictor.mlflow/#/experiments/2.\n",
      "2025-04-11 13:11:18,531 - INFO - All models trained, evaluated, and logged successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import dagshub\n",
    "import time\n",
    "import scipy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "# =====================================================================================================================\n",
    "\n",
    "def train_and_tune_model(X_train, y_train, model_name, model, param_grid):\n",
    "    \"\"\"Trains and tunes a given model using GridSearchCV.\"\"\"\n",
    "    \n",
    "    logging.info(f\"Training and tuning {model_name}...\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring=\"r2\", n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    logging.info(f\"Best {model_name} R² Score: {best_score}\")\n",
    "    logging.info(f\"Best {model_name} Parameters: {best_params}\")\n",
    "    \n",
    "    return best_model, best_score, best_params\n",
    "\n",
    "\n",
    "def evaluate_and_log_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluates the model and logs metrics to MLflow.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    logging.info(f\"{model_name} Evaluation - MAE: {mae}, MSE: {mse}, R² Score: {r2}\")\n",
    "\n",
    "    # Logging metrics to MLflow\n",
    "    mlflow.log_metric(f\"{model_name}_mae\", mae)\n",
    "    mlflow.log_metric(f\"{model_name}_mse\", mse)\n",
    "    mlflow.log_metric(f\"{model_name}_r2_score\", r2)\n",
    "\n",
    "    return mae, mse, r2\n",
    "\n",
    "\n",
    "# ========================== Defining functions for feature engineering pipeline =================================\n",
    "\n",
    "# scaling numerical columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def scale_numerical_features(df, numerical_columns, fit_scaler=False):\n",
    "    if fit_scaler:\n",
    "        df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "    else:\n",
    "        df[numerical_columns] = scaler.transform(df[numerical_columns])\n",
    "    return df\n",
    "\n",
    "# encoding categorical columns\n",
    "def encode_categorical_features(df, categorical_columns):\n",
    "    return pd.get_dummies(data=df, columns=categorical_columns)\n",
    "\n",
    "# feature-engineering pipeline\n",
    "def apply_feature_engineering(df, fit_scaler=False):\n",
    "    categorical_columns = ['bat_team', 'bowl_team']\n",
    "    numerical_columns = ['overs', 'runs', 'wickets', 'runs_last_5', 'wickets_last_5']\n",
    "    \n",
    "    df = encode_categorical_features(df, categorical_columns)\n",
    "    df = scale_numerical_features(df, numerical_columns, fit_scaler=fit_scaler)\n",
    "    return df\n",
    "\n",
    "\n",
    "#=================== Load the preprocessed data for our ease ========================\n",
    "''' When we save the preprocessed DataFrame to CSV and then reloaded it later using pd.read_csv(), \n",
    " the coverted [date] column goes back to string from datetime format, unless we re-parse it.'''\n",
    "\n",
    "df = pd.read_csv('preprocessed_data.csv', parse_dates=['date'])\n",
    "\n",
    "# Split using date (no random split)\n",
    "train_df = df[df['date'].dt.year <= 2015].copy()\n",
    "test_df = df[df['date'].dt.year >= 2016].copy()\n",
    "\n",
    "# Drop target column and split further into X & y / train & test sets\n",
    "X_train = train_df.drop(columns=['total'])\n",
    "y_train = train_df['total']\n",
    "\n",
    "X_test = test_df.drop(columns=['total'])\n",
    "y_test = test_df['total']\n",
    "logging.info(\"Splitting dataframe into train and test sets\")\n",
    "\n",
    "# Drop 'date' before encoding\n",
    "if 'date' in X_train.columns:\n",
    "    X_train = X_train.drop(columns='date') \n",
    "if 'date' in X_test.columns:                    \n",
    "    X_test = X_test.drop(columns='date')\n",
    "\n",
    "\n",
    "# Feature Engineering applied separately to train and test\n",
    "X_train = apply_feature_engineering(X_train, fit_scaler=True)      # fits and transforms train data\n",
    "X_test = apply_feature_engineering(X_test, fit_scaler=False)       # only transforms test data\n",
    "\n",
    "# Align columns of test set to match train set (fixing dummy mismatch)\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "logging.info(\"Applied Feature Engineering separately to train and test\")\n",
    "\n",
    "\n",
    "# Initialize MLflow\n",
    "mlflow.set_tracking_uri('https://dagshub.com/nayanparvez90/Innings-Score-Predictor.mlflow')\n",
    "dagshub.init(repo_owner='nayanparvez90', repo_name='Innings-Score-Predictor', mlflow=True)\n",
    "\n",
    "# mlflow set experiment\n",
    "mlflow.set_experiment(\"Model-Hyper-Parameter Tuning\")\n",
    "\n",
    "\n",
    "# Defining models and their hyperparameter grids\n",
    "models = {\n",
    "    \"LinearRegression\": (LinearRegression(), {                         # using all 3 coz r2-score nearly equal to eachother\n",
    "        \"fit_intercept\": [True, False],\n",
    "        \"positive\": [True, False],\n",
    "        \"n_jobs\": [-1]\n",
    "    }),\n",
    "\n",
    "    \"Lasso\": (Lasso(), {\n",
    "        \"alpha\": [0.01, 0.1, 1.0, 10.0],\n",
    "        \"fit_intercept\": [True, False],\n",
    "        \"positive\": [True, False],\n",
    "        \"max_iter\": [1000, 5000]\n",
    "    }),\n",
    "\n",
    "    \"Ridge\": (Ridge(), {\n",
    "        \"alpha\": [0.01, 0.1, 1.0, 10.0],\n",
    "        \"fit_intercept\": [True, False],\n",
    "        \"solver\": ['auto', 'svd', 'cholesky'],\n",
    "        \"max_iter\": [1000, 5000]\n",
    "    }),\n",
    "\n",
    "# use in-case these algorithms give better results when running algorithm-selection \n",
    "\n",
    "    # \"RandomForest\": (RandomForestRegressor(), {\n",
    "    #     \"n_estimators\": [50, 100, 200],\n",
    "    #     \"max_depth\": [None, 10, 20],                    \n",
    "    #     \"min_samples_split\": [2, 5, 10],\n",
    "    #     \"n_jobs\": [-1]\n",
    "    # }),\n",
    "\n",
    "    # \"XGBoost\": (XGBRegressor(objective=\"reg:squarederror\", eval_metric=\"rmse\"), {\n",
    "    #     \"n_estimators\": [50, 100, 200],\n",
    "    #     \"max_depth\": [3, 6, 10],\n",
    "    #     \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    #     \"n_jobs\": [-1]\n",
    "    # })\n",
    "}\n",
    "\n",
    "\n",
    "# initializing variables to store results\n",
    "best_model = None\n",
    "best_score = float(\"-inf\")\n",
    "best_model_name = \"\"\n",
    "best_model_metrics = {}\n",
    "\n",
    "# Train, evaluate, and log models separately\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_Run\"):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        best_trained_model, model_score, best_params = train_and_tune_model(X_train, y_train, model_name, model, param_grid)\n",
    "\n",
    "        mlflow.log_param(\"best_model\", model_name)\n",
    "        mlflow.log_params(best_params)                         # Log all best params here\n",
    "        \n",
    "        # Evaluate the trained model\n",
    "        mae, mse, r2 = evaluate_and_log_model(best_trained_model, X_test, y_test, model_name)\n",
    "        \n",
    "        # Log model to MLflow\n",
    "        input_example = X_test[:5] if not scipy.sparse.issparse(X_test) else X_test[:5].toarray()\n",
    "        mlflow.sklearn.log_model(best_trained_model, model_name, input_example=input_example)\n",
    "        logging.info(f\"{model_name} logged successfully in MLflow.\")\n",
    "\n",
    "# Log feature importances if available, it's optional u can comment this if block if no need for it\n",
    "        if hasattr(best_trained_model, \"feature_importances_\"):\n",
    "            try:\n",
    "                feature_importances = best_trained_model.feature_importances_\n",
    "                feature_names = X_train.columns if hasattr(X_train, \"columns\") else [f\"f{i}\" for i in range(len(feature_importances))]\n",
    "                \n",
    "                importance_df = pd.DataFrame({\n",
    "                    \"feature\": feature_names,\n",
    "                    \"importance\": feature_importances\n",
    "                }).sort_values(by=\"importance\", ascending=False)\n",
    "                \n",
    "                # Log as a CSV file\n",
    "                importance_path = f\"feature_importances_{model_name}.csv\"\n",
    "                importance_df.to_csv(importance_path, index=False)\n",
    "                mlflow.log_artifact(importance_path)\n",
    "\n",
    "                # Clean up temp files\n",
    "                os.remove(importance_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Could not log feature importances for {model_name}: {e}\")\n",
    "        \n",
    "        # Compare models and store the best one\n",
    "        if model_score > best_score:\n",
    "            best_score = model_score\n",
    "            best_model = best_trained_model\n",
    "            best_model_name = model_name\n",
    "            best_model_metrics = {\"mae\": mae, \"mse\": mse, \"r2\": r2}\n",
    "            best_model_params = best_params                            # Stores best params for final logging\n",
    "\n",
    "\n",
    "\n",
    "# Log the best model metrics separately\n",
    "with mlflow.start_run(run_name=\"Best_Model_Run\"):\n",
    "    mlflow.log_param(\"final_best_model\", best_model_name)\n",
    "    mlflow.log_params(best_model_params)                               #  Logs best params of the best model\n",
    "    mlflow.log_metric(\"final_best_mae\", best_model_metrics[\"mae\"])\n",
    "    mlflow.log_metric(\"final_best_mse\", best_model_metrics[\"mse\"])\n",
    "    mlflow.log_metric(\"final_best_r2_score\", best_model_metrics[\"r2\"])\n",
    "    \n",
    "    input_example = X_test[:5] if not scipy.sparse.issparse(X_test) else X_test[:5].toarray()\n",
    "    mlflow.sklearn.log_model(best_model, \"Best_Model\", input_example=input_example)\n",
    "\n",
    "    logging.info(f\"Final best model ({best_model_name}) logged successfully in MLflow.\")\n",
    "\n",
    "logging.info(\"All models trained, evaluated, and logged successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed80091",
   "metadata": {},
   "source": [
    "### Why the R² is different in both i.e. for eg. (Best Ridge R² Score: 0.58 and Ridge Evaluation: 0.68)\n",
    "\n",
    "- Best R² Score from GridSearchCV:\n",
    "\n",
    "-- This is the cross-validation score from the training data (X_train, y_train).\n",
    "\n",
    "-- It averages the performance across multiple training/validation folds.\n",
    "\n",
    "-- It’s used to choose the best hyperparameters during tuning.\n",
    "\n",
    "- Evaluation R² Score:\n",
    "\n",
    "-- This is calculated on the test set (X_test, y_test) i.e. data the model has never seen during training or tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a4ee6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

-------------------------Setting up project structure---------------------------

1. Create repo, clone it in local
2. Create a virtual environment named 'atlas' - conda create -n atlas python=3.10
3. Activate the virtual environment - conda activate atlas
4. pip install cookiecutter
5. cookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science
6. Rename src.models -> src.model
7. git add - commit - push


-------------------------Setup MLFlow on Dagshub---------------------------
8. Go to: https://dagshub.com/dashboard
9. Create > New Repo > Connect a repo > (Github) Connect > Select your repo > Connect
10. Copy experiment tracking url and code snippet. (Also try: Go To MLFlow UI)
11. pip install dagshub & mlflow

12. Run exp notebooks(a. Exploratory Data Analysis  / EDA.ipynb
                      b. Base-Regressor-model      / base_model.ipynb
                      c. Best-Performing-Algorithm/ model_selection.ipynb
                      d. Best-Model's-tuning     / model_tuning.ipynb) & visualize performance on mlflow-ui(sort by: user)
13. git add - commit - push

14. on terminal - "dvc init"
15. create a local folder as "local_s3" (temporary work/testing)
16. on terminal - "dvc remote add -d mylocal local_s3"

18. add file - dvc.yaml (till model evaluation.metrics)
19. add file - params.yaml
20. DVC pipeline is ready to run - dvc repro (on terminal:[ git rm -r --cached 'data/' ])
21. Once do - dvc status
22. git add - commit - push

23. Need to add S3 as remote storage - Create IAM User(keep cred) and S3 bucket
24. pip install - dvc[s3] & awscli
25. Checking/deleting dvc remote (optional) - [dvc remote list & dvc remote remove <name>] 
26. Set aws cred - aws configure
27. Add s3 as dvc remote storage - dvc remote add -d myremote s3://<bucket-name>

28. Create new dir - flaskapp | Inside that, add rest of the files and dir:
    files  : [app.py, prod_requirements.txt]
    folders: [static, templates]    

    -----------------------------for PRODUCTION purposes-------------------------------------
    note1 - params.yaml added coz we using it in our app in modular way and not hard-coding.
    note2 - logger file added here coz we used our custom logging in our app also.

29. Run the app and test it in your local system:
    - test if we're getting any error before the site is loaded
    - if the logic built in our app is working or failing (eg: same team bats and bowls)
    - use some test data from interim to see how the predictions are doing.

      bat_team        |      bowl_team   |  runs |  wickets | overs |  runs_last_5 |  wickets_last_5  |  total (actual)
Kolkata Knight Riders |  Mumbai Indians  |   55  |    1     |  7.3  |       42     |       1          |  187


30. pip freeze > requirements.txt



=========================================================================================================================

Now that our pipeline runs successfully and our app runs on localhost just fine,

We're going for a UI-friendly CI/CD pipeline on AWS and the goal is to minimize manual effort and 
leveraging AWS services through the console (UI, no CLI or coding unless necessary).

Plan of action; creating a CI/CD pipeline that:

a. Pulls code from our GitHub repo
b. Builds it with CodeBuild
c. Deploys it to an EC2 instance with CodeDeploy
d. Uses CodePipeline to glue everything together

- services we'll be using:
            Service	                            Purpose
            IAM	                 Set role-services for EC2 and CodeDeploy
            EC2	                    Run our web app / service
            S3	                 Store build artifacts (we don't even need to create-codepipeline takes care of it)
            CodeBuild	             Compile / test / package code
            CodeDeploy	              Deploy built artifacts to EC2
            CodePipeline	            Automates the entire flow

- Before moving to aws console we need to do some steps must be completed in order to smoothly deploy 
  our app on live server i.e. into production such as:

1. copy logger from src, paste it into our flaskapp dir
2. copy params.yaml, paste it into flaskapp dir

    -----------------------------for PRODUCTION purposes-------------------------------------
    note1 - params.yaml added coz we using it in our app in modular way and not hard-coding.
    note2 - logger file added here coz we used our custom logging in our app also.
            we'll use code ['from logger import logging'] in our app.py

3. git add > commit > push to main repository.

4. Enter AWS console login 
5. Create an IAM user and create 2 Service Roles:

(a). Code Deploy service role

Go to roles > create role > entity: AWS service > usecase: CodeDeploy > Next > Next >
Give the Role name > create role > open the role >
In Permissions policies > select add permissions(attach policies)    > select these 6 permissions:

i.   AmazonEC2FullAccess
ii.  AmazonS3FullAccess
iii. AWSCodeDeployFullAccess
iv.  AWSCodeDeployRole 
v.   AmazonEC2RoleforAWSCodeDeploy
vi.  AmazonEC2RoleforAWSCodeDeployLimited

(b). EC2-CodeDeploy connection service role (same steps as above except pick EC2 for usecase)    > select 3 permissions

i.   AmazonEC2FullAccess
ii.  AmazonS3FullAccess
iii. AWSCodeDeployFullAccess


6. Launch an EC2-instance:
- name it > pick ubuntu 22.04 version AMI >
- create key pair or use existing one (.pem) >
- create security group > allow both http & https > launch instance >
Now:
- select running instance > Go to securities > click on security groups >
- select Edit Inbound rules > Add rule > port range: 5000 > next to custom select 0.0.0.0/0 > save rules

note: port range depends on what we use in our app.py when defining port
Now again,

- select instance > Go to Actions > Security (Modify IAM role) >
- select IAM service role we created with EC2 usecase > update IAM role.

Now lastly,
- select insatnce > connect > connect 
- on connection establishment > write command 'vim install.sh' > enter

now, we're to insert a code in this empty file (code is given in ec2_script.txt file in our project so copy it) 

To Insert > press 'i'
To Paste  > right-click + paste
To Save & Exit > press 'esc' button 
                 press ':' 
                 write 'wq'        > enter

Back to our terminal in EC2 > write command 'bash install.sh' > enter


7. Search CodeDeploy > Getting Started > Create Application > Name it > pick EC2 as compute platform > create Application
Create Deployment Group > name it > add service role we created with CodeDeploy as usecase > Deployment type: In-place
Environment configuration : Amazon EC2 instances key: name , value: 'running-ec2-instance' > 
Install AWS CodeDeploy Agent : Never (due to updates mismatching in EC2 & CodeDeploy) > Disable load balancing > create deployment group


8. Now CodePipeline
Getting started > create pipeline > creation option: build custom pipeline > Next
pipeline settings stage
- name it
- execution mode : superseded
- service role and name: new (default) > Next

Add source stage 
- add source > github(via app) > create connection/use existing, to your github > pick repo > default branch main
Output artifact format: CodePipeline default > Next


add build stage:
Build provider: other( Aws CodeBuild )
create project > name it > Project type: Default > Environment: default all except Running mode; choose Instance not container

no change only in buildspec; select buildspec
and untick CloudWatch logs > Continue to CodePipeline


back to build stage > Next

add test stage > can skip it 

add deploy stage > Deploy provider: AWS CodeDeploy > here we give application name & deployment group name > Next > review > create pipeline


now our pipeline builds sourceartifact is input artifact for build stage and build artifact is input artifact for deploy stage 
on successful run of all stages go to ec2 instance connected terminal 

to checck if pipeline executed or not check list of dir- 'ls'
flaskapp appears
now use these steps to run app
cd /home/ubuntu/flaskapp > enter 
check for files & folders in flaskapp > 'ls'
we must find app.py, prod_requirements.txt, buildspec.yaml, appspec.ynl, params.yaml 

run pip3 install -r prod_requirements.txt
run python3 app.py

got a link to go to to give dagshub authorization like this:
Open the following link in your browser to authorize the client:
https://dagshub.com/login/oauth/authorize?state

select that link > right click + copy > open link pasting it on internet > allow access to dagshub for a month or day acc to our need

then our app works by itself > copy http link given by instance select it's middle ip > replace it with public ip on screen right below the terminal 

our app is up and running.
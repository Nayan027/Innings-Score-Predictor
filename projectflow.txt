-------------------------Setting up project structure---------------------------

1. Create repo, clone it in local
2. Create a virtual environment named 'atlas' - conda create -n atlas python=3.10
3. Activate the virtual environment - conda activate atlas
4. pip install cookiecutter
5. cookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science
6. Rename src.models -> src.model
7. git add - commit - push


-------------------------Setup MLFlow on Dagshub---------------------------
8. Go to: https://dagshub.com/dashboard
9. Create > New Repo > Connect a repo > (Github) Connect > Select your repo > Connect
10. Copy experiment tracking url and code snippet. (Also try: Go To MLFlow UI)
11. pip install dagshub & mlflow

12. Run exp notebooks(a. Exploratory Data Analysis  / EDA.ipynb
                      b. Base-Regressor-model      / base_model.ipynb
                      c. Best-Performing-Algorithm/ model_selection.ipynb
                      d. Best-Model's-tuning     / model_tuning.ipynb) & visualize performance on mlflow-ui(sort by: user)
13. git add - commit - push

14. on terminal - "dvc init"
15. create a local folder as "local_s3" (temporary work/testing)
16. on terminal - "dvc remote add -d mylocal local_s3"

18. add file - dvc.yaml (till model evaluation.metrics)
19. add file - params.yaml
20. DVC pipeline is ready to run - dvc repro (on terminal:[ git rm -r --cached 'data/' ])
21. Once do - dvc status
22. git add - commit - push

23. Need to add S3 as remote storage - Create IAM User(keep cred) and S3 bucket
24. pip install - dvc[s3] & awscli
25. Checking/deleting dvc remote (optional) - [dvc remote list & dvc remote remove <name>] 
26. Set aws cred - aws configure
27. Add s3 as dvc remote storage - dvc remote add -d myremote s3://<bucket-name>

28. Create new dir - flaskapp | Inside that, add rest of the files and dir:
    files  : [app.py, prod_requirements.txt]
    folders: [static, templates]    

    -----------------------------for PRODUCTION purposes-------------------------------------
    note1 - params.yaml added coz we using it in our app in modular way and not hard-coding.
    note2 - logger file added here coz we used our custom logging in our app also.

29. Run the app and test it in your local system:
    - test if we're getting any error before the site is loaded
    - if the logic built in our app is working or failing (eg: same team bats and bowls)
    - use some test data from interim to see how the predictions are doing.

      bat_team        |      bowl_team   |  runs |  wickets | overs |  runs_last_5 |  wickets_last_5  |  total (actual)
Kolkata Knight Riders |  Mumbai Indians  |   55  |    1     |  7.3  |       42     |       1          |  187


30. pip freeze > requirements.txt



=========================================================================================================================

Now that our pipeline runs successfully and our app runs on localhost just fine,

We're going for a UI-friendly CI/CD pipeline on AWS and the goal is to minimize manual effort and 
leveraging AWS services through the console (UI, no CLI or coding unless necessary).

Plan of action; creating a CI/CD pipeline that:

a. Pulls code from our GitHub repo
b. Builds it with CodeBuild
c. Deploys it to an EC2 instance with CodeDeploy
d. Uses CodePipeline to glue everything together

- services we'll be using:
            Service	                            Purpose
            IAM	                 Set role-services for EC2 and CodeDeploy
            EC2	                    Run our web app / service
            S3	                 Store build artifacts (we don't even need to create-codepipeline takes care of it)
            CodeBuild	             Compile / test / package code
            CodeDeploy	              Deploy built artifacts to EC2
            CodePipeline	            Automates the entire flow

- Before moving to aws console we need to do some steps must be completed in order to smoothly deploy 
  our app on live server i.e. into production such as:

1. copy logger from src, paste it into our flaskapp dir
2. copy params.yaml, paste it into flaskapp dir

    -----------------------------for PRODUCTION purposes-------------------------------------
    note1 - params.yaml added coz we using it in our app in modular way and not hard-coding.
    note2 - logger file added here coz we used our custom logging in our app also.
            we'll use code ['from logger import logging'] in our app.py

3. git add > commit > push to main repository.

4. Enter AWS console login 
5. Create IAM user & set up IAM Configuration (create 2 custom service roles)

(a) Create CodeDeploy Service Role
  Go to IAM > Roles > Create role
  Trusted entity: AWS service
  Use case: CodeDeploy
  Click Next until permissions screen
  Attach the following 6 policies:
      AmazonEC2FullAccess
      AmazonS3FullAccess
      AWSCodeDeployFullAccess
      AWSCodeDeployRole
      AmazonEC2RoleforAWSCodeDeploy
      AmazonEC2RoleforAWSCodeDeployLimited
      Name the role appropriately
  Click Create role

(b) Create EC2-CodeDeploy Service Role
  Same steps as above, but choose EC2 as use case
  Attach these 3 policies:
      AmazonEC2FullAccess
      AmazonS3FullAccess
      AWSCodeDeployFullAccess
  Name and create the role

6. Launch EC2 Instance
  Name the instance
  AMI: Ubuntu 22.04
  Create or use existing .pem key pair
  Create a security group:
          Allow HTTP (80)
          Allow HTTPS (443)
  Launch instance

  a. Configure Security Group for Custom Port (e.g., 5000):
  Go to Running Instances > Security > Security Groups > Edit Inbound Rules
  Add rule:
  Port range: 5000
  Source: 0.0.0.0/0

  b. Attach IAM Role to EC2:
  select running instance > Go to Actions > Security > Modify IAM role
  Attach the EC2 service role created earlier

7. EC2 Initial Setup
  Connect to the instance via browser-based SSH
  Run: vim install.sh

  In install.sh:
  Press i to insert
  Paste content from ec2_script.txt   (don't use ctrl+v for pasting here do it manually)

  Save & Exit:
  ESC, then :wq, then ENTER

  Then run: bash install.sh



8. CodeDeploy Service 
  a.create CodeDeploy Application
    Go to CodeDeploy > Getting Started > Create Application
    Name it
    Compute platform: EC2

  b.Create Deployment Group:
    Name it
    Attach CodeDeploy service role
    Deployment type: In-place
    Environment config:
      EC2 tag key: name,   Value: <running-ec2-instance>
    Install agent: Choose Never (to avoid version mismatch)
    Disable load balancing
    Click Create Deployment Group
    on creation of deployment group now we move to CodePipeline to automate the entire workflow

9. Setup CodePipeline
  a.Creation stage
  Getting started > create pipeline > creation option: build custom pipeline > Next

  b.Pipeline Settings Stage
    Pipeline name
    Execution mode: Superseded
    Service role: New (default)
    Click Next

  c.Source Stage
    Provider: GitHub (via app)
    Create connection / use existing
    Choose repository and main branch
    Output artifact format: CodePipeline default

  d.Build Stage
    Provider: AWS CodeBuild
    Create project:
    Name the project
    Project type: Default
    Environment: All default EXCEPT:
    Running mode: Instance (not container)
    Buildspec: select option to USE buildspec.yaml from source
    Untick CloudWatch logs
    Click Continue to CodePipeline

  e.Test Stage
    Can be skipped

  f.Deploy Stage
    Deploy provider: AWS CodeDeploy
    Choose the application name and deployment group from earlier
    Review and create pipeline

10.Pipeline Execution
  Once all stages execute i.e. Sourece --> Build --> Deploy then,
  Go to EC2 terminal, check list of directories:
  run: ls
  You should see the directory flaskapp in the list

11.Run Your App
  run: cd /home/ubuntu/flaskapp    
  run: ls                   

  Check existence of necessary files and folders to run app:
  app.py, prod_requirements.txt, templates, static, buildspec.yaml, appspec.yaml, params.yaml

  run: pip3 install -r prod_requirements.txt
  run: python3 app.py

12.Authorize with DAGsHub (If prompted)
  Copy the DAGsHub OAuth link
  Open in browser
  Authorize access

13.Access the Running App
  From EC2 console:
  Copy the public IP
  Replace the 127.0.0.1 or internal IP in terminal link with EC2 public IP

Visit in browser:    http://<EC2-public-IP>:5000